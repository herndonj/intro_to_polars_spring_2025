{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BpXYam9cSrW"
      },
      "source": [
        "# Introduction to Polars using Python\n",
        "\n",
        "This workshop is part of the [CDVS Spring 2025 workshop series](library.duke.edu/data/workshops). If you have questions about the content or would like to schedule a consulation with Duke Libraries [Center for Data and Visualization Sciences](library.duke.edu/data), please contact the instructor: Joel Herndon (joel.herndon@duke.edu)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aO-0rCV1eEun"
      },
      "source": [
        "**Abstract**  \n",
        "Polars provides an open source library for data manipulation that allows fast, large-scale data work in multiple data analytics languages without the need for specialized computing. This workshop focuses on using the Polars library in the Python programming language. We will spend workshop time covering the context for Polars, common data manipulation processes, and close by considering possibilities of using Polars as part of a larger data workflow.Â This event is open to non-Duke participants."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efif8Mfpq8Z0"
      },
      "source": [
        "## Why Polars?\n",
        "\n",
        "Why another data manipulation language?\n",
        "\n",
        "1. Speed (especially on laptops/personal machines)\n",
        "2. Strictness/Consistency\n",
        "3. Expressiveness"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Workshop goals and guidelines\n",
        "\n",
        "- This workshop is designed to make you comfortable with Polars fundamentals. The goal of the workshop is to provide enough detail to enable you to evaluate whether Polars is a good fit for your work.\n",
        "  - We will use fairly simple examples, but Polars is capable of much more complex work.\n",
        "  - We won't cover all the features available in Polars in this hour long workshop, but I hope you continue to explore!\n",
        "  - We won't consider all aspects of the Polars API, but I list some resources at the end that will provide details.\n",
        "\n",
        "- We are using Google's Colab in the live workshop, but I'd encourage everyone to consider a full featured code development environment if you wish to use polars further.\n",
        "\n",
        "- You are encouraged to ask questions, both during the workshop. We're also happy to stay in touch at askdata@duke.edu after the workshop!"
      ],
      "metadata": {
        "id": "7rKJuXuQMkLd"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnWC0dSsefaF"
      },
      "source": [
        "## Core data structures\n",
        "\n",
        "Polars has two prominent data structures.\n",
        "\n",
        "1. Series\n",
        "2. DataFrames\n",
        "\n",
        "### 1. Series\n",
        "\n",
        "- named, one-dimensional data structures\n",
        "- containing sequential data\n",
        "- all data are of the same type (homogenous)\n",
        "\n",
        " Key aspects of a polars Series:\n",
        "\n",
        "- It is immutable (operations create new Series rather than modifying existing ones)\n",
        "- It is designed for fast vectorized operations\n",
        "- It can handle null (missing) values while maintaining strong typing\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's build two small polars Series comprised of:\n",
        "\n",
        "1. the names of coffee drinks\n",
        "2. the prices of coffee drinks"
      ],
      "metadata": {
        "id": "YQB_F1t1T8Co"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6PKQru-Z6U2"
      },
      "outputs": [],
      "source": [
        "# 1. coffee drink orders\n",
        "import polars as pl\n",
        "coffee_drink_orders = pl.Series('coffee_drinks', ['brewed coffee', 'latte', 'mocha', 'espresso', 'latte', 'mocha'])\n",
        "coffee_drink_orders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YF7I3tkhYETa"
      },
      "outputs": [],
      "source": [
        "# 2. prices of coffee drinks\n",
        "import polars as pl\n",
        "coffee_drink_prices = pl.Series('coffee_prices', [2.25, 4.25, 5.50, 2.25,4.25, 5.50])\n",
        "coffee_drink_prices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDjnuSJkZCS5"
      },
      "source": [
        "### 2. DataFrames\n",
        "\n",
        "- named, two-dimensional data structures\n",
        "- containing a collection of Series (columns)\n",
        "- each column (polars Series) can hold a different data type\n",
        "- each column (polars Series) must contain the same number of elements\n",
        "\n",
        "Key aspects of a polars DataFrame:\n",
        "\n",
        "- It is heterogeneous (different columns can have different data types)\n",
        "- Columns are named\n",
        "- It is immutable (operations create new DataFrames rather than modifying existing ones)\n",
        "- The columns are internally stored as contiguous arrays, optimized for both row and column operations\n",
        "- It supports vectorized operations across multiple columns\n",
        "\n",
        "Since we have already have two polars Series (coffee drinks and coffee prices) with information about the coffee names and coffee prices, we can build a polars DataFrame from the two Series that we just created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYIj4n1SaiJt"
      },
      "outputs": [],
      "source": [
        "# DataFrame of names and prices of coffee drinks\n",
        "coffee_df = pl.DataFrame(\n",
        "    {\n",
        "        'name' : coffee_drink_orders,\n",
        "        'price': coffee_drink_prices\n",
        "    }\n",
        ")\n",
        "coffee_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AH7nKnrpxt7"
      },
      "source": [
        "Another popular approach for creating DataFrame in code is to utilize a group of python dictionaries create the Series that comprise the polars DataFrame.\n",
        "\n",
        "The following code combines four dictionaries into a new DataFrame called `flights`.\n",
        "\n",
        "I created this small dataframe of various flights from Raleigh Durham airport (RDU) so that we can explore Polars data wrangling in a way where you can see the transformations clearly. We'll use this dataset in some common data manipulation tasks that follow.\n",
        "\n",
        "Note: the distance variables is courtesy of: [https://www.airportdistancecalculator.com/](https://www.airportdistancecalculator.com/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvaO1YC-qbMC"
      },
      "outputs": [],
      "source": [
        "import polars as pl\n",
        "flights = pl.DataFrame(\n",
        "    {\n",
        "        \"flight\": [\"Raleigh Durham to Svalbard\",\n",
        "                   \"Raleigh Durham to London\",\n",
        "                   \"Raleigh Durham to Los Angeles\",\n",
        "                   \"Raleigh Durham to Boston\",\n",
        "                   \"Raleigh Durham to Chicago\",\n",
        "                   \"Raleigh Durham to New York\"],\n",
        "        \"cost\": [None,1200.00,600.00,250.00,300.00, 420.00],\n",
        "        \"distance_km\": [6209,6216,3595,985,1040,693],\n",
        "        \"non_stop\": [False,True,True, True, False, True],\n",
        "    }\n",
        ")\n",
        "flights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmMYLzhTFdlY"
      },
      "source": [
        "## Common Polars Data Types\n",
        "\n",
        "Polars has a wide range of [data types](https://docs.pola.rs/api/python/stable/reference/datatypes.html).\n",
        "\n",
        "For today's workshop, we will focus on:\n",
        "\n",
        "- Numeric types (on my mac the default is 64 bit floats (decimals) and 64 bit integers)\n",
        "- Strings\n",
        "- Booleans\n",
        "\n",
        "If you look at the output of the `flights` DataFrame above, you can see that Polars will assign default types to the data as they are assigned.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtyoejJ-bJXf"
      },
      "source": [
        "## Loading and Saving Data\n",
        "\n",
        "Generally, most of the data that you are going to use in Polars won't be created inside your program.\n",
        "\n",
        "Polars has a broad set of tools for loading and parsing data stored in a variety of common formats."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cir3OFinYB8h"
      },
      "source": [
        "##### Delimited Files\n",
        "\n",
        "Polars' `read_csv()` function will read comma delimited files when you provide a location for the file. Polars also uses the `read_csv()` function to load other delimited formats when specify the delimeter using the `separator` argument (see example below).\n",
        "\n",
        "Let's give it a try.\n",
        "\n",
        "When you execute the code block below, click the browse button below the code, and upload the `flights.csv` file in the data directory that you downloaded earlier in the class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTsCyCWmYSkd"
      },
      "outputs": [],
      "source": [
        "# if you are running this outside of google colab\n",
        "# omit the first two lines\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "df = pl.read_csv(\"flights.csv\") # basic example\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTfWEPCGbso7"
      },
      "source": [
        "In the example above, we had a relatively simple polars DataFrame with clean data. Sometimes, your data may have certain content that requires additional specification to load the data cleanly. Some common examples follow:\n",
        "\n",
        "1. Your data have missing values encoded as \"NA\" (a string), but should be treated as missing data.  \n",
        "`df = read_csv(\"flights.csv\", null_values=\"NA\")`\n",
        "\n",
        "2. Your data are delimited, but instead of using a comma, use a tab (\\t) or other delimiter.  \n",
        "`df = read_csv(\"flights.txt\", separator=\"\\t\") # tab delimited code`\n",
        "\n",
        "3. Your data contain dates and/or dates with times and you would like for Polars to attempt to convert those fields to the appropriate date or time type.  \n",
        "`df = read_csv(\"flights.csv\", try_parse_dates=True)`\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jckpNP4JYKct"
      },
      "source": [
        "##### Excel\n",
        "\n",
        "No matter how you feel about Microsoft Excel, it is a very popular format. You can directly load a single sheet of excel data using Polars `read_excel` command.\n",
        "    \n",
        "**Note:** Polars uses a helper package to read excel files. In the example that follows, Google Colab appears to require the `fastexcel` package to be installed for this example to work. Please uncomment the first line to install `fastexcel` if needed. If you are using another development environment, please install the necessary helper package. See the [Polars API notes](https://docs.pola.rs/api/python/stable/reference/api/polars.read_excel.html).\n",
        "\n",
        "When you execute the code block, click browse, and upload the `flights.xlsx` file in the data directory that you downloaded earlier in the class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qKHwJPIQaOpv"
      },
      "outputs": [],
      "source": [
        "#!pip install fastexcel\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "excel_flights = pl.read_excel(\"flights.xlsx\") #basic example\n",
        "excel_flights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V90spdRZ1urn"
      },
      "source": [
        "### Saving Data\n",
        "\n",
        "In general most of the functions for loading data have a corresponding function for writing data as well. Please see the [Polars API](https://docs.pola.rs/api/python/stable/reference/io.html) for further details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwmFuVxm12KM"
      },
      "outputs": [],
      "source": [
        "df.write_csv(\"flights.csv\") #basic example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kq1jZWWAF3Fh"
      },
      "source": [
        "### Missing data in Polars\n",
        "\n",
        "Polars represents missing values using `null` exclusively across data types. When you are loading data, remember that it's important to ensure that other encodings for missing data are converted so that they are correctly represented as missing (and not as strings (\"NA\"), negative numbers (-99), or some other encoding)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpKlQy6oFThP"
      },
      "source": [
        "# Common Data Manipulation Processes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BdNQONCoXLH"
      },
      "source": [
        "One of Polars great strengths as a data manipulation library is its simple, expressive syntax that allows you to construct complex expressions by combining a simple elements. We will discuss Polars expressions shortly, but let's try some hands-on data data wrangling first."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6W25fZhVF-Cq"
      },
      "source": [
        "### 5 common \"verbs\" for data wrangling in Polars\n",
        "\n",
        "Inspired by a common way of sharing the common commands for data wrangling in the R data analysis langauge, I present five common methods for data wrangling using Polars.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psyxgxfxfKN6"
      },
      "source": [
        "#### 1. select()\n",
        "\n",
        "The `select` command is the primary method for choosing columns (or variables) that you wish to include in a dataframe. Using our `flights` DataFrame, let's look at a few examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmf53pa5iEkM"
      },
      "outputs": [],
      "source": [
        "# First, let's remind ourselves of the columns that are available.\n",
        "# I can do this several ways. Try: `flights.columns` for example.\n",
        "# Let's look at the entire DataFrame\n",
        "flights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoMM-MGnnYmN"
      },
      "source": [
        "If I want to see the names of the flights, I can select the `flight` column using the `pl.col` function which tells polars which columns that we wish to select. We will use `pl.col` to select columns in all of these examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tbm2Ejonni78"
      },
      "outputs": [],
      "source": [
        "flights.select(pl.col('flight'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oru6baWRkSDW"
      },
      "source": [
        "If I wanted to see the `flight` and the `distance_km` columns only, I can select those to appear."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nw_eVA7IiwjA"
      },
      "outputs": [],
      "source": [
        "flights.select(pl.col('flight','distance_km'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3l4nRthklTo"
      },
      "source": [
        "You are welcome to select as many columns as you wish, but when you have a lot of columns and you want almost all of them, polars offers the `drop()` method to remove columns that you wish to omit. Let's drop the airport abbreviations in the `origin` and `destination` columns. Typing one variable is a lot faster than typing three!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7uK_pEmkP0y"
      },
      "outputs": [],
      "source": [
        "flights\n",
        "flights.drop(pl.col('cost')) # note that I am not saving this change into a new DataFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OX59OIWjlXKT"
      },
      "source": [
        "`select()` is a powerful tool. By reducing the number of columns in your dataframe, you increase the efficiency of your analysis (and increase the speed).\n",
        "  \n",
        "Note: Polars offers some additional options for selecting columns.\n",
        "\n",
        "- It is possible to use regular expressions to define a selection `flights.select(pl.col('^c.*$'))` # columns starting with \"c\"\n",
        "- It is possible to select by data type\n",
        "- It is possible to use slice notation and avoid select entirely (but this is not preferred syntax) `flights[:]`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EgWScAti2Zp"
      },
      "source": [
        "#### 2. filter()\n",
        "The `filter` command is the primary method for selecting rows based on certain criteria that you wish to include in a dataframe.\n",
        "\n",
        "Polars filters rows using a column (or vector) of true/false values to determine which rows to keep and which rows to drop (technically this is called \"boolean mask filtering\").\n",
        "\n",
        "Using our flights DataFrame, let's look at a few examples.\n",
        "\n",
        "Let's say that we wanted only the flights from RDU that were of a distance less than 1000 kilometers. We could reduce our DataFrame to these flights with the following code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-Zu4JmBkljd"
      },
      "outputs": [],
      "source": [
        "flights.filter(pl.col('distance_km') < 1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJMMMvMR7P6C"
      },
      "source": [
        "So, our Boston and New York flights are both under 1000 kilometers from Durham.  \n",
        "\n",
        "Let's make this a bit more complex, by adding more criteria to the filter.\n",
        "\n",
        "Polars allows complex filter queries by combining multiple conditions using boolean expressions. For most operations, you will use the following operators to build these expressions.\n",
        "\n",
        "|Operator | Symbol |\n",
        "|----------|:----------:|\n",
        "| AND   | &   |\n",
        "| OR    | \\|   |\n",
        "| NOT   | ~    |\n",
        "\n",
        "\n",
        "\n",
        "Polars tends to require explicit (with a few shortcuts- see below) conditions- so expect to type in each condition. Also, Polars wants you to enclose each condition in parentheses. It's really helpful to use a development environment that provides visual cues (usually colored parentheses) or code tips to ensure you've closed the relevant parenthesis.\n",
        "\n",
        "\n",
        "Let's say that we want to see all the flights that are:\n",
        "\n",
        "1. less than 6000 kilometers from Durham\n",
        "2. less than $500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAvI20Sl9D7t"
      },
      "outputs": [],
      "source": [
        "flights.filter(\n",
        "    (pl.col('distance_km')<6000) &\n",
        "    (pl.col('cost')< 500.00)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gevqgJR_7qa"
      },
      "source": [
        "When a column contains a boolean data type (`true` or `false`), you can simply add it into the set of conditions (if the item is `true`, include that row). For example:  \n",
        "\n",
        "Which flights in our dataset are non-stop from Raleigh Durham?  I know that I have non-stop column that has boolean values (they are `true` or `false` depending on whether the flight is non-stop).?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w17NkYgEAdWv"
      },
      "outputs": [],
      "source": [
        "flights.filter(pl.col('non_stop'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p34yABILAqu2"
      },
      "source": [
        "Since the column is already contains boolean values, simply listing the column as a filter condition selects the rows where that condition evaluates to `True`.  \n",
        "\n",
        "\n",
        "**BONUS**: What if you wanted to see the flights with multiple stops?    \n",
        "\n",
        "\n",
        "Not a problem!\n",
        "\n",
        "Just use `NOT` boolean operator `~` like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFxe3nUgCEEV"
      },
      "outputs": [],
      "source": [
        "flights.filter(~pl.col('non_stop'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qk9T0bOICOXp"
      },
      "source": [
        "#### 3. sort()\n",
        "\n",
        "In the previous exercises, `select` and `filter` have helped to reduce our Dataframe to relevant columns and rows.\n",
        "\n",
        "The `sort` method builds on these commands by allowing us to specify how the columns should be arranged for presentation.\n",
        "\n",
        "Let's try a simple sort based on our filtered data considering non-stop flights in the dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZaCKo4-JE5M8"
      },
      "outputs": [],
      "source": [
        "flights.filter(pl.col('non_stop')).sort(pl.col('distance_km'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AJzNG4eWao8"
      },
      "source": [
        "In the above, note how `sort` defaults to ascending order. If you want to sort in descending order:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bNEpD276hYs"
      },
      "outputs": [],
      "source": [
        "flights.filter(pl.col('non_stop')).sort(pl.col('distance_km'), descending=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YC2PfDTv6hx9"
      },
      "source": [
        "It is entirely possible to specify more than one column for a sort. Let's look at our non-stop flights that have multiple stops and sort by the cost and the distance_km."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gOZQVdaWJfJp"
      },
      "outputs": [],
      "source": [
        "flights.filter(pl.col('non_stop')).sort([\n",
        "    pl.col('cost'),\n",
        "    pl.col('distance_km')])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UrX2FlYWtSN"
      },
      "source": [
        "*Optional*: Is it possible to sort some columns ascending with others descending? Yes, just assign bracketed list of booleans (`True` or `False`) to each of the sort columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lX6_N3p_W4Hl"
      },
      "outputs": [],
      "source": [
        "flights.filter(pl.col('non_stop')).sort([\n",
        "    pl.col('cost'),\n",
        "    pl.col('distance_km')],\n",
        "    descending=[True, False])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuN-i2McFKS3"
      },
      "source": [
        "Note how our data are now sorted by the distance in kilometers in ascending order. Often, we will want to see the results in descending order (top 5 lists are very popular in data work!). It is easy to specify the `descending` parameter in the `sort` method as `True` to do this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXCgKWjwVvxd"
      },
      "source": [
        "One last thing to not about sorting DataFrames.  If you have missing data in a sort column, it will be listed first (it doesn't matter if you are sorting ascending or descending). If you would prefer to have the `null` values appear at the end of the sort, add the `nulls_last=True` parameter.\n",
        "\n",
        "Nulls first:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrIopkiFZXwi"
      },
      "outputs": [],
      "source": [
        "flights.sort(pl.col('cost'), descending=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kvqBty7aKXL"
      },
      "source": [
        "Nulls last:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ss1u4f-AaJrY"
      },
      "outputs": [],
      "source": [
        "flights.sort(pl.col('cost'), descending=True, nulls_last=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBTtPhNlJXu5"
      },
      "source": [
        "It is entirely possible to specify more than one column for a `sort`. Let's look at our flights that have multiple stops (non-stop = False) and sort by the `cost` and the `distance_km`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHCUPXDsWQDR"
      },
      "source": [
        "#### 4. with_columns()\n",
        "\n",
        "We often want to build on the data that we have in our dataset by adding addtional variables to our datasets. The `with_columns()` method provides a convenient way to Adding columns to DataFrames.  Let's calculate the cost per kilometer for each of flights and try to determine which is the best bargain.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ucyeKTrxd7ot"
      },
      "outputs": [],
      "source": [
        "flights.with_columns(\n",
        "   cost_per_kilometer = pl.col('cost') / pl.col('distance_km')\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7H-8B-3e-o0"
      },
      "source": [
        "The flight to Los Angeles is a slightly better deal in the cost per kilometer than the flight to London. Note that the we didn't have a cost for the flight to Svalbard (we only have a `null` value there) and we couldn't calculate the cost per kilometer for that trip.\n",
        "\n",
        "Up until now, we've been selecting, filtering, and sorting our data without making any changes in the underlying DataFrame. As we start adding new variables (or new columns) like the \"cost per kilometer\", we will likely want to save our new column. In order to do this, you must assign the result of the `.with_columns()` expression to a new (or existing) variable name like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEsFp7dwgr7u"
      },
      "outputs": [],
      "source": [
        "flights = flights.with_columns(\n",
        "   cost_per_kilometer = pl.col('cost') / pl.col('distance_km')\n",
        ")\n",
        "flights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e397DQWQhezS"
      },
      "source": [
        "Another stylistic note- Polars offers two different syntax styles for creating columns.\n",
        "\n",
        "1. *keyword argument syntax*: In the example above, we name the column, apply the `=` sign and define our calculation.  \n",
        "\n",
        "2. *expression based syntax*: Polars also has an expression based syntax that assigns the name of the variable by using an `.alias()` method at the end of the expression. This may seem more intuitive if you use python a lot and enjoy method chaining. (If that doesn't make sense- stick with the keyword argument syntax!)\n",
        "\n",
        "```\n",
        "flights = flights.with_columns(\n",
        "   (pl.col('cost') / pl.col('distance_km')).alias('cost_per_kilometer')\n",
        ")\n",
        "flights\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_1MSDwfbYgC"
      },
      "source": [
        "#### 5. group_by()\n",
        "\n",
        "\n",
        "Sometimes you want to aggregate your data to understand patterns by groups. The `group_by()` method provides a convenient way to collapse the dataframe into groups defined by one or more variables and then generate statistics based on these groupings. Note that `group_by()` on it's own only splits the data into groups (see code below) without producing results, we have to add some more code to run calculations on the groups.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "flights.group_by(pl.col('non-stop')) # this creates groups... but no output!"
      ],
      "metadata": {
        "id": "B62JizyqZKwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For now, we will start with a very simple example using our small flights dataset that considers the average distance of non-stop flights versus flights with layovers. The code after `.agg()` calculates the mean of the distance in kilometers and renames the calculated variable `average_distance_km`."
      ],
      "metadata": {
        "id": "aEuNYW_BZFqD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLLgbjd1pyhb"
      },
      "outputs": [],
      "source": [
        "flights.group_by(pl.col('non_stop')).agg(pl.col('distance_km').mean().alias('average_distance_km'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIPXSoU-10Wq"
      },
      "source": [
        "#### 6. over()\n",
        "\n",
        "I know. I said I was only going to show you five common \"verbs\" in Polars...\n",
        "\n",
        " Some of you are probably looking at the `group_by()` function and thinking \"that's really great for aggregating the data and creating summaries by categories.... BUT what if I want to aggregate the data by groups and then add that calculation to each row that is a member of that group?  This is where the `over()` method comes in handy in Polars. Let's try the same example as above, but this time we'll use `.over` in our expression instead of `group_by()`.\n",
        "\n",
        " In the output that follows, you can see how average distance is calculated for each group in the non-stop variable (`true` and `false`) and then added to the Dataframe as a new column. We use polars `.alias` method to provide a name for the newly calculated column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KqBmFkB6y_3"
      },
      "outputs": [],
      "source": [
        "flights.with_columns(pl.col('distance_km').mean().over(pl.col('non_stop')).alias('average_distance_km'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nV93E_Zebhml"
      },
      "source": [
        "### Expressions\n",
        "\n",
        "Let us focus on a core feature of polars: **expressions**. We've been using expressions already in our small data examples without noting it. As Jannsens and Nieuwdorp (2025) describe it:\n",
        "\n",
        "> \"If you think of an expression as a recipe, then the operations would be the steps, and the functions and methods would be like the cooks.\"\n",
        "\n",
        "Jannsens and Nieuwdorp (2025) also provide a formal definition:\n",
        "\n",
        "> \"An expression is a tree of operations that describe how to constuct one or more Series.\"\n",
        "\n",
        "Let's look at an example to make this clearer. In the code below, we use the DataFrame method `with_columns` to add a column to the DataFrame. In this example, you can think of of `with_columns` as the chef and the expression that follows as the steps to make it happen. Indeed, the steps or the expression is completely reusable in a variety of contexts. Let's try it!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# run this code for reminder of how with_columns works\n",
        "# we've seen this earlier in the workshop\n",
        "flights.with_columns(\n",
        "   cost_per_kilometer = pl.col('cost') / pl.col('distance_km')\n",
        ")"
      ],
      "metadata": {
        "id": "fjcaURIDHZ_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now, let's extract the expression into a variable\n",
        "my_expression = cost_per_kilometer = pl.col('cost') / pl.col('distance_km')"
      ],
      "metadata": {
        "id": "R4F_VEjlIxju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "My instructions for creating a `cost_per_kilometer` are now saved in the variable `my_expression`. This is valid code, but without the DataFrame and a method (\"verb\") to activate the code, if I call the expression I just see the code."
      ],
      "metadata": {
        "id": "Ma2f9_ZQRnNK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note that the expression is valid code (you won't get an error)\n",
        "# BUT also note that you need a DataFrame and one of our methods (\"verbs\") that\n",
        "# acts on the expression to generate a result.\n",
        "my_expression"
      ],
      "metadata": {
        "id": "fRrJcsn_K7fu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's use our expression in a different context. I'm still working with the same `flights` data that we used before, but this time I'm going to use the `select` method of the DataFrame. Note that you can use `select` to generate Series in a DataFrame as well as \"selecting\" Series in DataFrame."
      ],
      "metadata": {
        "id": "ftWF8nTISB3y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# and now, I'm going to reuse my_expression with the same Dataframe\n",
        "# but with a different verb (or method)\n",
        "flights.select(\n",
        "    my_expression\n",
        ")"
      ],
      "metadata": {
        "id": "me_sS9EjJIWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If this seems confusing, don't worry! Expressions tend to make more sense as you use Polars more frequently."
      ],
      "metadata": {
        "id": "lzyvL8A2Ha_2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6N9FEfMNPgG3"
      },
      "source": [
        "## Polars Workflows\n",
        "\n",
        "### Big data and Polars\n",
        "\n",
        "The six observations in the flights dataset provide a quick visual idea of polars data manipulation tools, but do not illustrate polars ability to process large data collections. For this section of the workshop, we will scale up our current flights dataset to examine air travel data to Raleigh Durham airport from 2010 to 2024 using data from the Federal Aviation Administration (FAA). A special thanks to Simon Couch and his colleagues who developed the [R package \"anyflights\"](https://github.com/simonpcouch/anyflights) that provided the tools for downloading the FAA data!\n",
        "\n",
        "Let us load these data into Colab and explore Polars with a dataset of roughly a million rows. When you execute the code that follows, click the browse button that appears, and upload the `rdu_flights.parquet` file in the data directory that you downloaded earlier in the class. Note that we are using the [parquet format](https://en.wikipedia.org/wiki/Apache_Parquet) which stores very large files efficiently. When the file loads, you will note that we get a report of the first five rows and the last five rows of the DataFrame. Polars is designed for working for large data- so the default response takes the size of the data into account."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TuwLzHO4bkf3"
      },
      "outputs": [],
      "source": [
        "# if you are running this outside of google colab\n",
        "# omit the first two lines\n",
        "import polars as pl #if neeeded\n",
        "from google.colab import files\n",
        "files.upload() # choose rdu_flights.parquet\n",
        "rdu_flights = pl.read_parquet(\"rdu_flights.parquet\") # basic example\n",
        "rdu_flights"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting a sense of the data\n",
        "Before we start exploring the `rdu_flights` data, let us first try to understand a little bit about how this data differs from our six flight example that we used earlier. If you look at the top of the of the DataFrame listing above, you will note that Polars already provides information about the DataFrame each time you print it. The shape (954_988, 45) tells us that we have 954988 rows (the underscore is python's way of making the number easier to read) and 45 columns.  It is possible to ask polars directly for the number of rows and columns by using the `height` and `width` properties of the DataFrame at any time. We can also ask for the column names (`columns`) to get a sense of the variables.\n",
        "\n",
        "Note: If you are using polars heavily for data exploration, I'd strongly recommend using an intergrated development environment (IDE) for coding and data analysis. Several IDEs have powerful and convenient tools for exploring data and variables that will allow you to focus on analytics."
      ],
      "metadata": {
        "id": "r-vtz3wG2a2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdu_flights.height # how many rows in the dataframe?"
      ],
      "metadata": {
        "id": "rsuG8Sdk6BoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdu_flights.width # how many columns?"
      ],
      "metadata": {
        "id": "3W11jhK_6Iwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdu_flights.columns # what are the column names?"
      ],
      "metadata": {
        "id": "a8yxWwnO6Zpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdu_flights.schema # let's look at the column names and data types together"
      ],
      "metadata": {
        "id": "-z62KJVPA8QM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdu_flights.describe() # descriptive statistics for the dataframe (you have to look at the dep_delay column)"
      ],
      "metadata": {
        "id": "sYMJYTq4rA4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `rdu_flights` DataFrame list all the flights with Raleigh Durhgam airport as the `origin`. The DataFrame also has rich information about the nature of the flight (departure time versus scheduled departure for example), the weather at RDU at the time of the flight, and the type of plane used for the flight. For todays workshop, we only have time to explore a bit, but you are welcome to use this dataset to explore (and maybe form some hypothesis about?) flights out of Raleigh Durham airport.\n",
        "\n",
        "### Exploratory Analysis: Example 1\n",
        "\n",
        "The primary reason that most people consider polars is its speed at processing large data collections. Let's ask some questions about our data and note the speed with roughly a million observations.\n",
        "\n",
        "1. What are the number of departing flights at RDU airport by year?\n",
        "\n",
        "We can't directly see a million observations, but we might be interested in the number of flights that originate from Raleigh Durham airport each year from 2010 to 2024 (we only have data through September 2024). Let's group the DataFrame by year and create an expression to count the flights (each row in this DataFrame is a flight) using the `len()` method."
      ],
      "metadata": {
        "id": "pUHMP1RK6pVz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egoQl6-tPNfo"
      },
      "outputs": [],
      "source": [
        "rdu_flights.group_by('year').agg(pl.len())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is great... the polars algorithm is fast (instantaneous), but it doesn't return the data sorted in a way that makes it easy to parse. We'd like to have those years sorted.\n",
        "\n",
        "I'll also note that polars default way of printing DataFrames is great for looking at big data, but very frustrating for smaller DataFrames, we'll change the default.  If you use this code: `pl.Config.set_tbl_rows(100)`, Polars will display up to 100 rows without breaks."
      ],
      "metadata": {
        "id": "3lQ894v7Yws5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pl.Config.set_tbl_rows(100) # show 100 rows\n",
        "rdu_flights.group_by('year').agg(pl.len()).sort('year')"
      ],
      "metadata": {
        "id": "OEJLd6dLazti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is great, but we might want to share this with someone. Let us rename the second column from `len` to `flights` How would you do that?"
      ],
      "metadata": {
        "id": "mk7bDN_DbSBS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdu_flights.group_by(pl.col('year')).agg(pl.len().alias('flights')).sort(pl.col('year'))"
      ],
      "metadata": {
        "id": "dLJYOHpwbeSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is very handy. I can see the flights increasing at RDU up to the pandemic, and a steady recovery in the last few years (remember, we still don't have a full year of data for 2024). This calculation is so handy, that I'm going to save this expression `pl.len.alias('flights')` so I can use it later without having to retype the entire expression."
      ],
      "metadata": {
        "id": "mNTtm0-Wlts7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "number_of_flights = (pl.len().alias('flights'))"
      ],
      "metadata": {
        "id": "v0ruHSYQmHIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploratory Analysis: Example 2\n",
        "\n",
        "2. Are departure delays increasing or decreasing at RDU each year?\n",
        "\n",
        "We have a variable called `dep_delay` that measures delays in minutes (I note that some flights seem to leave early because there are negative delays). Let's count the number of rows each year where the departure delay is greater than 15 minutes."
      ],
      "metadata": {
        "id": "xPDIQJnJfmed"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdu_flights.group_by('year').agg(\n",
        "    pl.col('dep_delay')\n",
        "    .filter(pl.col('dep_delay') > 15)\n",
        "    .len()\n",
        "    .alias('delayed_flights')\n",
        ").sort('year')"
      ],
      "metadata": {
        "id": "XbNsevmqbhmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If I'm a researcher for the FAA (or RDU) interested in flight delay data, I might want to use that expression again later. Let's save it in a variable called \"flights_delayed_by_15_minutes\".\n"
      ],
      "metadata": {
        "id": "4jJ0VQv4i93k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flights_delayed_by_15_minutes = (\n",
        "    pl.col('dep_delay')\n",
        "    .filter(pl.col('dep_delay') > 15)\n",
        "    .len()\n",
        "    .alias('delayed_flights')\n",
        ")\n"
      ],
      "metadata": {
        "id": "soO4qRHDkWVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's used our saved expression (`flights_delayed_by_15_minutes`) in the aggregation (`.agg()`) part of our grouping."
      ],
      "metadata": {
        "id": "a8nTXnnZf9X_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdu_flights.group_by('year').agg(\n",
        "    flights_delayed_by_15_minutes\n",
        ").sort('year')"
      ],
      "metadata": {
        "id": "-TfrahJ3f8Vp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploratory Analysis: Example 3\n",
        "\n",
        "3. Is there some relationship with increases in the number of flights and delays?\n",
        "\n",
        "We've already calculated the number of flights per year and saved that expression (`number_of_flights`).\n",
        "\n",
        "We've also saved the expression that calculates the flights delayed by fifteen minutes (`flights_delayed_by_15_minutes`).\n",
        "\n",
        "We can run the same analysis that groups the data by year and simply provide those two expressions to see our flights and delays together.\n",
        "\n",
        "The `.agg` function will accept a python list (`[]`) of expressions and run each calculation for us.\n",
        "\n",
        "Let's give it a try:"
      ],
      "metadata": {
        "id": "WLSSboJgkWy5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdu_flights.group_by('year').agg([\n",
        "    number_of_flights,\n",
        "    flights_delayed_by_15_minutes]\n",
        ").sort('year')"
      ],
      "metadata": {
        "id": "llIv4AYMjINi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploratory Analysis: Example 4 (if we have time)\n",
        "\n",
        "4. What's the best day of the week in 2014 to fly to Washington, DC (Ronald Reagan National Airport or DCA) if you want the best chance of departing on time?\n",
        "\n",
        "Translation to Polars\n",
        "\n",
        "1. We want flights in 2014: `filter(pl.col('year') = 2014)`\n",
        "2. We want flights to DCA so: `filter(pl.col('dest') == \"DCA\")`\n",
        "3. We also want flights that depart on time so: `filter(pl.col('dep_delay') <= 0)`\n",
        "\n",
        "As we learned earlier, polars allows us to combine these conditions in a single filter command. If the filters are additive (each condition is linked by an AND), polars will allow to list the conditions separated by commas. It will know that all the conditions must apply.\n",
        "\n",
        "\n",
        "4. We haven't covered dates in polars yet, but we have a variable called `departure_date_time` that is a polars `datetime` type. This allows us to query that field for the numerical day of the week (Monday is 1 and Sunday is 7) using this code: `pl.col('departure_date_time).dt.weekday()`. We'll add this to our DataFrame using `with_columns()`.\n",
        "\n",
        "5. In order to calculate the best chance of of departing on time (on any airline) to Reagan National, we need to calculate the average number of on time (or early departures) on average for each day of the week. This calculation requires two groupings.\n",
        "  1. Our first `group_by` creates a DataFrame with the day of the week, the date and the number of flights.\n",
        "  2. The second `group_by` groups the DataFrame from the previous step by day and calculates the average of on-time flights for that day.\n",
        "\n",
        "Let's put these together in code and see what we get.\n"
      ],
      "metadata": {
        "id": "tofPVvToiUoM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Coding challenge: can you convert the numeric day?\n",
        "rdu_flights.filter(\n",
        "    pl.col('year') == 2024,\n",
        "    pl.col('dest') == \"DCA\",\n",
        "    pl.col('dep_delay') <= 0\n",
        ").with_columns(\n",
        "    pl.col('departure_date_time').dt.weekday().alias('day_of_week')\n",
        ").group_by([pl.col('day_of_week'), pl.col('departure_date_time').dt.date()]).agg(\n",
        "    pl.len().alias('daily_flights')\n",
        ").group_by('day_of_week').agg(\n",
        "    pl.col('daily_flights').mean().alias('avg_daily_on_time_flights')\n",
        ").sort('avg_daily_on_time_flights', descending = True)"
      ],
      "metadata": {
        "id": "Okf1qWvtvK_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on 2024 flights, your best bet is flying on Tuesday!"
      ],
      "metadata": {
        "id": "WfVHPxvAAXe2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Polars Lazy (and extremely fast) API\n",
        "\n",
        "Up to this point, we have used Polars eager application programming interface (API) to process our queries. When we enter commands, we get immediate feedback. This immediate feedback resembles other data manipulation libraries like Pandas (python) and dplyr (R), and should be familar if you have worked in those languages.\n",
        "\n",
        "Polars *lazy API* provides an alternative way of processing large data or working in high performance workflows. Instead of loading data and immediately executing transfomations, Polars evaluates the DataFrame noting the characteristics of the data without loading the data and builds a query graph of the transformations requested. By deferring immediate execution and not loading data until called (the \"lazy\" in Polars \"lazy\" API), the lazy API can significantly optimize data manipulation queries.\n",
        "\n",
        "We won't spend a lot of time looking at Lazy API in today's workshop, but we explore some of its core commands here.\n",
        "\n",
        "### Using the Lazy API\n",
        "\n",
        "Implementing the lazy API, generally begins with creating a \"LazyFrame\" (or representation of the DataFrame that does not contain the data). If you have a DataFrame in memory, you can use the `lazy()` method to convert it to a LazyFrame and use the Polars lazy API."
      ],
      "metadata": {
        "id": "IBiau553JA0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lazy_rdu_flights = rdu_flights.lazy()"
      ],
      "metadata": {
        "id": "3V8kTOL4htaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's also possible to scan data into a Polars LazyFrame using one of Polars many \"scan\" methods."
      ],
      "metadata": {
        "id": "LCXEkkobZzmS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lazy_rdu_flights = pl.scan_parquet(\"rdu_flights.parquet\")"
      ],
      "metadata": {
        "id": "9045TzM52Q-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you have loaded your data into a lazyframe, the data manipulation steps are identical to the ones that we have used above with one exception - when you start with a LazyFrame, Polars will not provide a DataFrame in the output until you \"materialize\" the DataFrame using the `collect()`.\n",
        "\n",
        "In the examples that follow. The first example uses polars eager API (as we did earlier) to evaluate our query. The second example uses the same query with the lazy API.  I've timed each query (I note that I get varying results on Google Colab...), but generally the lazy API is faster. If you have very big data (more than one million rows and many columns), you should definitely consider the lazy API as part of your analytics strategy!"
      ],
      "metadata": {
        "id": "2kVzPcpPaN6i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets use the Polars eager API\n",
        "%time\n",
        "rdu_flights.group_by('year').agg([\n",
        "    number_of_flights,\n",
        "    flights_delayed_by_15_minutes]\n",
        ").sort('year')"
      ],
      "metadata": {
        "id": "0F_9FS2k0QJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets use the lazy API\n",
        "%time\n",
        "lazy_rdu_flights.group_by('year').agg(\n",
        "    [\n",
        "        number_of_flights,\n",
        "        flights_delayed_by_15_minutes\n",
        "    ]).sort('year').collect()\n"
      ],
      "metadata": {
        "id": "q1m1yJCd0yWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Closing Thoughts and Questions\n",
        "\n"
      ],
      "metadata": {
        "id": "9qiwlDozFvbx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting Help\n",
        "\n",
        "Learning Polars is an iterative process that takes time. It can be frustrating to translate your analysis goals into a new language-- especially if you are very skilled in a given language. The following resources may be of help:  \n",
        "\n",
        "### General resources\n",
        "\n",
        "- *CDVS - Duke Libraries - askdata@duke.edu*  \n",
        "As always, Duke Libraries Center for Data and Visualization Science (askdata@duke.edu) can assist with questions about data management and data wrangling. Consultations are available bny\n",
        "\n",
        "- *Polars API*  \n",
        "While I sometimes disagree with how things are organized... the [Polars Python API](https://docs.pola.rs/api/python/stable/reference/index.html) is an outstanding resource for the latest syntax. If you are questioning the validity of AI suggestions (and sometimes those suggestions are erroneous!), the API can help resolve questions.\n",
        "\n",
        "### LLM (\"AI\") Resources\n",
        "- [Claude.ai](claude.ai) is quite good at code related questions (although you should always verify what it tells you!).\n",
        "- [Microsoft's Co-Pilot](https://oit.duke.edu/service/microsoft-copilot/) is available at Duke and can be helpful.\n",
        "- [Github for Education](https://github.com/education) (*Students and Educators* by application) - Github offers very generous access to its  tools which include github copilot.\n",
        "\n",
        "### eBooks\n",
        "Duke Libraries subscribe to the [O'Reilly for Higher Education](https://go.oreilly.com/duke-university) Database where you will find Jeroen Janssens and Thijs Nieuwdorp's [Python Polars: The Definitive Guide](https://learning.oreilly.com/library/view/python-polars-the/9781098156077/). This is an excellent way to learn Polars and serves as compelling reference. Note that the book will be published (in print!) in April 2025.\n",
        "\n",
        "### Data resources\n",
        "We used FAA flight data pulled using the [anyflights]() API. If you would like to investigate this API further check out:  \n",
        "\n",
        "Couch S (2023). anyflights: Query 'nycflights13'-Like Air Travel Data for Given Years and Airports. https://github.com/simonpcouch/anyflights, https://simonpcouch.github.io/anyflights/.\n",
        "\n",
        "### Other polars APIs\n",
        "\n",
        "As mentioned in the introduction, [Polars](https://pola.rs/) is written in Rust and has implementations in Julia, R (check out [TidyPolars](https://tidypolars.etiennebacher.com/), and Javascript. If Polars seems compelling and you regularly use one of those languages, please try one of the other implementations!\n",
        "\n"
      ],
      "metadata": {
        "id": "5BcFqEratDU5"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}